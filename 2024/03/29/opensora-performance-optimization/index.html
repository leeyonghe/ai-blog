<!DOCTYPE html>
<html lang="en">

<head><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

<!-- 기본 SEO 메타 태그 -->
<title>Open-Sora 성능 최적화 완벽 가이드: CUDA 메모리부터 Flash Attention까지 | AI Development Blog</title>
<meta name="description" content="개요AI 비디오 생성은 막대한 계산 리소스를 요구합니다. Open-Sora는 11B 파라미터 모델로 768px 비디오를 생성할 때 80GB 이상의 GPU 메모리가 필요할 수 있습니다. 이번 포스트에서는 Open-Sora가 사용하는 다양한 성능 최적화 기법을 상세히 분석해보겠습니다....">
<meta name="author" content="David Lee">
<meta name="keywords" content="opensora, cuda, flash-attention, xformers, memory-optimization, gpu, performance-tuning">

<!-- 정규 URL -->
<link rel="canonical" href="https://leeyonghe.github.io/ai-blog/2024/03/29/opensora-performance-optimization/">

<!-- Open Graph / Facebook -->
<meta property="og:type" content="article">
<meta property="og:title" content="Open-Sora 성능 최적화 완벽 가이드: CUDA 메모리부터 Flash Attention까지">
<meta property="og:description" content="개요AI 비디오 생성은 막대한 계산 리소스를 요구합니다. Open-Sora는 11B 파라미터 모델로 768px 비디오를 생성할 때 80GB 이상의 GPU 메모리가 필요할 수 있습니다. 이번 포스트에서는 Open-Sora가 사용하는 다양한 성능 최적화 기법을 상세히 분석해보겠습니다....">
<meta property="og:url" content="https://leeyonghe.github.io/ai-blog/2024/03/29/opensora-performance-optimization/">
<meta property="og:site_name" content="AI Development Blog">

<meta property="og:image" content="https://leeyonghe.github.io/ai-blog/assets/images/opensora-performance-optimization.png">

<meta property="og:locale" content="ko_KR">

<!-- Twitter Card -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="Open-Sora 성능 최적화 완벽 가이드: CUDA 메모리부터 Flash Attention까지">
<meta name="twitter:description" content="개요AI 비디오 생성은 막대한 계산 리소스를 요구합니다. Open-Sora는 11B 파라미터 모델로 768px 비디오를 생성할 때 80GB 이상의 GPU 메모리가 필요할 수 있습니다. 이번 포스트에서는 Open-Sora가 사용하는 다양한 성능 최적화 기법을 상세히 분석해보겠습니다....">

<meta name="twitter:image" content="https://leeyonghe.github.io/ai-blog/assets/images/opensora-performance-optimization.png">



<!-- 검색 엔진 인증 메타 태그 -->



<!-- 언어 및 지역 설정 -->
<meta name="language" content="ko">
<meta name="geo.region" content="KR">
<meta name="geo.country" content="KR">

<!-- 추가 SEO 메타 태그 -->
<meta name="robots" content="index, follow">
<meta name="revisit-after" content="7 days">
<meta name="rating" content="general">

<!-- RSS 피드 -->
<link rel="alternate" type="application/rss+xml" title="AI Development Blog" href="https://leeyonghe.github.io/ai-blog/feed.xml">

<!-- 구조화된 데이터 (JSON-LD) -->
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Blog",
  "name": "AI Development Blog",
  "description": "AI 개발 및 프레임워크 분석을 다루는 기술 블로그입니다. Rust, Python, AI/ML 프로젝트의 심층 분석과 실무 경험을 공유합니다.
",
  "url": "https://leeyonghe.github.io/ai-blog",
  "author": {
    "@type": "Person",
    "name": "David Lee",
    "email": "lee.yonghee.dev@gmail.com"
  },
  "inLanguage": "ko",
  "potentialAction": {
    "@type": "SearchAction",
    "target": "https://leeyonghe.github.io/ai-blog/search?q={search_term_string}",
    "query-input": "required name=search_term_string"
  }
}
</script>

<!-- 개별 포스트 구조화된 데이터 -->

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Open-Sora 성능 최적화 완벽 가이드: CUDA 메모리부터 Flash Attention까지",
  "description": "개요

AI 비디오 생성은 막대한 계산 리소스를 요구합니다. Open-Sora는 11B 파라미터 모델로 768px 비디오를 생성할 때 80GB 이상의 GPU 메모리가 필요할 수 있습니다. 이번 포스트에서는 Open-Sora가 사용하는 다양한 성능 최적화 기법을 상세히 분석해보겠습니...",
  "image": "https://leeyonghe.github.io/ai-blog/assets/images/opensora-performance-optimization.png",
  "author": {
    "@type": "Person",
    "name": "David Lee"
  },
  "publisher": {
    "@type": "Organization",
    "name": "AI Development Blog",
    "logo": {
      "@type": "ImageObject",
      "url": ""
    }
  },
  "datePublished": "2024-03-29T04:00:00+00:00",
  "dateModified": "2024-03-29T04:00:00+00:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://leeyonghe.github.io/ai-blog/2024/03/29/opensora-performance-optimization/"
  },
  "url": "https://leeyonghe.github.io/ai-blog/2024/03/29/opensora-performance-optimization/",
  "inLanguage": "ko",
  "keywords": ["opensora","cuda","flash-attention","xformers","memory-optimization","gpu","performance-tuning"],
  "articleSection": ["AI","Performance","GPU Computing"]
}
</script>

<link href="https://fonts.googleapis.com/css?family=Merriweather:300|Raleway:400,700" rel="stylesheet">
<link rel="stylesheet" href="/ai-blog/assets/css/style.css">
<link rel="stylesheet" href="/ai-blog/assets/css/post-detail.css">
<style>
/* Post Detail Page Styling */
.post-container article {
  background: #fff;
  border-radius: 12px;
  box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
  padding: 40px;
  margin: 30px auto;
  max-width: 800px;
}

@media screen and (max-width: 768px) {
  .post-container article {
    margin: 20px;
    padding: 25px;
    border-radius: 8px;
  }
}

.post-container article .post-header {
  border-bottom: 2px solid #f5f5f5;
  padding-bottom: 25px;
  margin-bottom: 30px;
}

.post-container article .post-header .post-title {
  font-size: 2.2em;
  font-weight: 700;
  color: #2c3e50;
  line-height: 1.3;
  margin-bottom: 15px;
}

@media screen and (max-width: 768px) {
  .post-container article .post-header .post-title {
    font-size: 1.8em;
  }
}

.post-container article .post-header .post-meta {
  display: flex;
  align-items: center;
  gap: 20px;
  flex-wrap: wrap;
}

.post-container article .post-header .post-meta .post-date {
  display: flex;
  align-items: center;
  color: #666;
  font-size: 0.9em;
}

.post-container article .post-header .post-meta .post-date i {
  margin-right: 8px;
  color: #888;
}

.post-container article .post-header .post-meta .post-categories-wrapper {
  display: flex;
  align-items: center;
}

.post-container article .post-header .post-meta .post-categories-wrapper i {
  margin-right: 8px;
  color: #888;
}

.post-container article .post-header .post-meta .post-categories-wrapper .post-categories {
  display: flex;
  gap: 8px;
  list-style: none;
  margin: 0;
  padding: 0;
}

.post-container article .post-header .post-meta .post-categories-wrapper .post-categories li {
  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
  color: white;
  padding: 4px 12px;
  border-radius: 20px;
  font-size: 0.8em;
  font-weight: 500;
  text-transform: capitalize;
}

.post-container article .post-content {
  font-size: 1.1em;
  line-height: 1.8;
  color: #333;
}

.post-container article .post-content p {
  margin-bottom: 1.2em;
}

.post-container article .post-content h1,
.post-container article .post-content h2,
.post-container article .post-content h3,
.post-container article .post-content h4,
.post-container article .post-content h5,
.post-container article .post-content h6 {
  color: #2c3e50;
  margin: 1.5em 0 0.8em 0;
  font-weight: 600;
}

.post-container article .post-content h2 {
  font-size: 1.8em;
  border-bottom: 2px solid #3498db;
  padding-bottom: 10px;
}

.post-container article .post-content h3 {
  font-size: 1.5em;
  color: #34495e;
}

.post-container article .post-content code {
  background: #f8f9fa;
  padding: 2px 6px;
  border-radius: 4px;
  font-size: 0.9em;
  color: #e74c3c;
}

.post-container article .post-content pre {
  background: #f8f9fa;
  border: 1px solid #e9ecef;
  border-radius: 6px;
  padding: 15px;
  overflow-x: auto;
  margin: 1.5em 0;
}

.post-container article .post-content pre code {
  background: none;
  color: #333;
  padding: 0;
}

.post-container article .post-content blockquote {
  border-left: 4px solid #3498db;
  background: #f8f9fa;
  padding: 15px 20px;
  margin: 1.5em 0;
  font-style: italic;
  color: #555;
}

.post-container article .post-content img {
  max-width: 100%;
  height: auto;
  border-radius: 8px;
  margin: 1.5em 0;
  box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
}

.post-container article .post-content ul,
.post-container article .post-content ol {
  padding-left: 1.5em;
  margin-bottom: 1.2em;
}

.post-container article .post-content ul li,
.post-container article .post-content ol li {
  margin-bottom: 0.5em;
}

.post-container article .post-content a {
  color: #3498db;
  text-decoration: none;
  border-bottom: 1px solid transparent;
  transition: all 0.3s ease;
}

.post-container article .post-content a:hover {
  border-bottom-color: #3498db;
}

.post-container article .post-footer {
  border-top: 2px solid #f5f5f5;
  padding-top: 25px;
  margin-top: 40px;
}

.post-container article .post-footer .post-navigation {
  display: flex;
  justify-content: space-between;
  align-items: center;
  gap: 20px;
}

@media screen and (max-width: 768px) {
  .post-container article .post-footer .post-navigation {
    flex-direction: column;
    gap: 15px;
  }
}

.post-container article .post-footer .post-navigation .nav-previous,
.post-container article .post-footer .post-navigation .nav-next {
  flex: 1;
}

.post-container article .post-footer .post-navigation .nav-previous a,
.post-container article .post-footer .post-navigation .nav-next a {
  display: flex;
  align-items: center;
  padding: 15px 20px;
  background: #f8f9fa;
  border-radius: 8px;
  text-decoration: none;
  color: #333;
  transition: all 0.3s ease;
  border: 2px solid transparent;
}

.post-container article .post-footer .post-navigation .nav-previous a:hover,
.post-container article .post-footer .post-navigation .nav-next a:hover {
  background: #e9ecef;
  border-color: #3498db;
  transform: translateY(-2px);
}

.post-container article .post-footer .post-navigation .nav-previous a i,
.post-container article .post-footer .post-navigation .nav-next a i {
  color: #3498db;
  margin: 0 8px;
}

.post-container article .post-footer .post-navigation .nav-previous a .nav-title,
.post-container article .post-footer .post-navigation .nav-next a .nav-title {
  font-weight: 500;
}

.post-container article .post-footer .post-navigation .nav-previous a {
  justify-content: flex-start;
}

.post-container article .post-footer .post-navigation .nav-next a {
  justify-content: flex-end;
  text-align: right;
}
</style>
<title>Open-Sora 성능 최적화 완벽 가이드: CUDA 메모리부터 Flash Attention까지</title>

<script type="text/javascript" src="/ai-blog/assets/js/darkmode.js"></script>


<!-- Mermaid Diagram Support -->
<script type="text/javascript" src="/ai-blog/assets/js/mermaid-init.js"></script>
<link rel="stylesheet" href="/ai-blog/assets/css/network-diagrams.css">
<style>
/* Mermaid diagram styling - Network optimized */
.mermaid {
  text-align: center;
  margin: 2em 0;
  padding: 1.5em;
  background: linear-gradient(135deg, #f8fafc 0%, #f1f5f9 100%);
  border: 2px solid #e2e8f0;
  border-radius: 12px;
  box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
  overflow-x: auto;
  position: relative;
}

.mermaid::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  height: 4px;
  background: linear-gradient(90deg, #3b82f6, #1d4ed8, #1e40af);
  border-radius: 12px 12px 0 0;
}

.mermaid svg {
  max-width: 100%;
  height: auto;
  filter: drop-shadow(0 2px 4px rgba(0, 0, 0, 0.1));
}

/* Network diagram specific styling */
.mermaid .node rect,
.mermaid .node circle,
.mermaid .node ellipse,
.mermaid .node polygon {
  stroke-width: 2px;
  filter: drop-shadow(0 1px 3px rgba(0, 0, 0, 0.12));
}

.mermaid .edgePath path {
  stroke-width: 2px;
  filter: drop-shadow(0 1px 2px rgba(0, 0, 0, 0.1));
}

.mermaid .cluster rect {
  stroke-width: 2px;
  stroke-dasharray: 5,5;
  opacity: 0.9;
}

/* Enhanced error styling */
.mermaid-error {
  color: #dc2626;
  background: linear-gradient(135deg, #fef2f2 0%, #fee2e2 100%);
  border: 2px solid #fca5a5;
  border-radius: 12px;
  padding: 1.5em;
  text-align: center;
  font-style: italic;
  font-weight: 500;
  box-shadow: 0 4px 20px rgba(220, 38, 38, 0.1);
}

.mermaid-error::before {
  content: '⚠️ ';
  font-size: 1.2em;
  margin-right: 0.5em;
}

/* Loading animation */
.mermaid.loading {
  min-height: 200px;
  background: linear-gradient(
    90deg,
    #f1f5f9 25%,
    #e2e8f0 50%,
    #f1f5f9 75%
  );
  background-size: 200% 100%;
  animation: shimmer 2s infinite;
}

@keyframes shimmer {
  0% {
    background-position: -200% 0;
  }
  100% {
    background-position: 200% 0;
  }
}

/* Responsive Mermaid diagrams */
@media (max-width: 768px) {
  .mermaid {
    font-size: 0.85em;
    padding: 1em;
    margin: 1.5em 0;
    border-radius: 8px;
  }

  .mermaid svg {
    transform: scale(0.9);
    transform-origin: center;
  }
}

@media (max-width: 480px) {
  .mermaid {
    font-size: 0.75em;
    padding: 0.8em;
    margin: 1em 0;
  }

  .mermaid svg {
    transform: scale(0.8);
  }
}

/* Dark mode support */
@media (prefers-color-scheme: dark) {
  .mermaid {
    background: linear-gradient(135deg, #1e293b 0%, #334155 100%);
    border-color: #475569;
    color: #f1f5f9;
  }

  .mermaid::before {
    background: linear-gradient(90deg, #60a5fa, #3b82f6, #2563eb);
  }

  .mermaid-error {
    background: linear-gradient(135deg, #451a03 0%, #7c2d12 100%);
    border-color: #dc2626;
    color: #fef2f2;
  }
}

/* Print styles */
@media print {
  .mermaid {
    background: white !important;
    border: 1px solid #ccc !important;
    box-shadow: none !important;
    break-inside: avoid;
  }

  .mermaid::before {
    display: none !important;
  }
}
</style>
</head><body>
  <main class="container">
    <section class="about">
      <div class="about-header condensed">
      <div class="about-title">
      <a href="/ai-blog/">
        
        <img src="/ai-blog/assets/portfolio.png" alt="David Lee" />
        
      </a>
      <h2 id="title">
        <a href="/ai-blog/">David Lee</a>
      </h2>
      </div><p class="tagline">Developer</p></div>
      
      <ul class="social about-footer condensed"><a href="https://github.com/leeyonghe" target="_blank">
          <li>
            <i class="icon-github-circled"></i>
          </li>
        </a><a href="https://www.linkedin.com/in/lee-yong-hee-18912454/" target="_blank">
          <li>
            <i class="icon-linkedin-squared"></i>
          </li>
        </a><!----></ul><p class="about-footer condensed">&copy;
        2025</p><div class="about-footer condensed">
        <p>Dark Mode
          <i class="icon-moon"></i>
          <label class="switch">
            <input type="checkbox" class="dark-mode-toggle">
            <span class="slider round" onclick="toggleDarkMode()"></span>
          </label>
        </p>
      </div>
    </section>
    <section class="content">
      <style>
/* Post Detail Page Styling - Direct Inline */
.post-container article {
  background: #fff !important;
  border: none !important;
  border-top: none !important;
  border-right: none !important;
  border-bottom: none !important;
  border-left: none !important;
  border-width: 0 !important;
  border-style: none !important;
  outline: none !important;
  border-radius: 12px !important;
  box-shadow: none !important;
  padding: 40px !important;
  margin: 30px auto !important;
  max-width: 800px !important;
}

.post-container article .post-header .post-title {
  font-size: 2.2em !important;
  font-weight: 700 !important;
  color: #2c3e50 !important;
  line-height: 1.3 !important;
}

.post-container article .post-header {
  border-bottom: none !important;
  padding-bottom: 25px !important;
  margin-bottom: 30px !important;
}

.post-container article .post-content {
  font-size: 1.1em !important;
  line-height: 1.8 !important;
  color: #333 !important;
}

.post-container article .post-content h2 {
  font-size: 1.8em !important;
  border-bottom: none !important;
  padding-bottom: 10px !important;
  color: #2c3e50 !important;
}

.post-container article .post-header .post-meta .post-categories li {
  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;
  color: white !important;
  padding: 4px 12px !important;
  border-radius: 20px !important;
  font-size: 0.8em !important;
  list-style: none !important;
  display: inline-block !important;
  margin-right: 8px !important;
}

.post-container article .post-footer {
  border-top: none !important;
  padding-top: 25px !important;
  margin-top: 40px !important;
}

/* 모든 border와 box-shadow 강제 제거 */
.post-container article *,
.post-container article *::before,
.post-container article *::after {
  border: none !important;
  border-top: none !important;
  border-right: none !important;
  border-bottom: none !important;
  border-left: none !important;
  border-width: 0 !important;
  border-style: none !important;
  outline: none !important;
  box-shadow: none !important;
}

/* 코드 블록과 pre 태그도 border, box-shadow 제거 */
.post-container article pre,
.post-container article code,
.post-container article .highlight,
.post-container article .highlighter-rouge {
  border: none !important;
  outline: none !important;
  box-shadow: none !important;
}
</style><div class="post-container">
  <article>
    <header class="post-header">
    <h1 class="post-title">Open-Sora 성능 최적화 완벽 가이드: CUDA 메모리부터 Flash Attention까지</h1>
    <div class="post-meta">
      <div class="post-date">
        <i class="icon-calendar"></i>
        <time datetime="2024-03-29T04:00:00+00:00">Mar 29, 2024</time>
      </div><div class="post-categories-wrapper">
        <i class="icon-tag"></i>
        <ul class="post-categories"><li>AI</li><li>Performance</li><li>GPU Computing</li></ul>
      </div></div>
  </header>

  <div class="post-content">
    <h2 id="개요">개요</h2>

<p>AI 비디오 생성은 막대한 계산 리소스를 요구합니다. Open-Sora는 11B 파라미터 모델로 768px 비디오를 생성할 때 80GB 이상의 GPU 메모리가 필요할 수 있습니다. 이번 포스트에서는 Open-Sora가 사용하는 다양한 성능 최적화 기법을 상세히 분석해보겠습니다.</p>

<h2 id="메모리-최적화-전략">메모리 최적화 전략</h2>

<h3 id="1-cuda-메모리-할당-최적화">1. CUDA 메모리 할당 최적화</h3>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Dockerfile에서의 CUDA 메모리 설정</span>
<span class="k">ENV</span><span class="s"> PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:8,expandable_segments:True,garbage_collection_threshold:0.95</span>
<span class="k">ENV</span><span class="s"> PYTORCH_NO_CUDA_MEMORY_CACHING=1</span>
<span class="k">ENV</span><span class="s"> CUDA_MEMORY_FRACTION=0.4</span>
<span class="k">ENV</span><span class="s"> PYTORCH_CUDA_ALLOC_CONF=backend=cudaMallocAsync,max_split_size_mb:16</span>
</code></pre></div></div>

<h4 id="각-설정의-효과">각 설정의 효과</h4>

<p><strong>max_split_size_mb:8</strong></p>
<ul>
  <li><strong>목적</strong>: 메모리 조각화 방지</li>
  <li><strong>효과</strong>: 큰 메모리 블록을 8MB 단위로 분할</li>
  <li><strong>장점</strong>: 메모리 부족 에러 감소</li>
</ul>

<p><strong>expandable_segments:True</strong></p>
<ul>
  <li><strong>목적</strong>: 동적 메모리 확장</li>
  <li><strong>효과</strong>: 필요에 따라 메모리 세그먼트 확장</li>
  <li><strong>장점</strong>: 메모리 사용량 최적화</li>
</ul>

<p><strong>garbage_collection_threshold:0.95</strong></p>
<ul>
  <li><strong>목적</strong>: 적극적인 가비지 컬렉션</li>
  <li><strong>효과</strong>: 메모리 사용률 95% 도달 시 정리</li>
  <li><strong>장점</strong>: 메모리 누수 방지</li>
</ul>

<p><strong>cudaMallocAsync 백엔드</strong></p>
<ul>
  <li><strong>목적</strong>: 비동기 메모리 할당</li>
  <li><strong>효과</strong>: 메모리 할당/해제 속도 향상</li>
  <li><strong>장점</strong>: GPU 아이들 타임 감소</li>
</ul>

<h3 id="2-데이터-타입-최적화">2. 데이터 타입 최적화</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 모델 정밀도 최적화
</span><span class="n">vae</span> <span class="o">=</span> <span class="n">vae</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">).</span><span class="nb">eval</span><span class="p">()</span>
<span class="n">stdit</span> <span class="o">=</span> <span class="n">stdit</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">).</span><span class="nb">eval</span><span class="p">()</span>

<span class="c1"># T5는 fp32 유지 (정확도 중요)
</span><span class="n">text_encoder</span><span class="p">.</span><span class="n">t5</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="p">.</span><span class="n">t5</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="bfloat16-vs-float32-vs-float16">bfloat16 vs float32 vs float16</h4>

<table>
  <thead>
    <tr>
      <th>데이터 타입</th>
      <th>메모리 사용량</th>
      <th>속도</th>
      <th>정확도</th>
      <th>수치 안정성</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>float32</strong></td>
      <td>100% (기준)</td>
      <td>1.0x</td>
      <td>⭐⭐⭐⭐⭐</td>
      <td>⭐⭐⭐⭐⭐</td>
    </tr>
    <tr>
      <td><strong>bfloat16</strong></td>
      <td>50%</td>
      <td>1.5-2x</td>
      <td>⭐⭐⭐⭐</td>
      <td>⭐⭐⭐⭐</td>
    </tr>
    <tr>
      <td><strong>float16</strong></td>
      <td>50%</td>
      <td>1.5-2x</td>
      <td>⭐⭐⭐</td>
      <td>⭐⭐⭐</td>
    </tr>
  </tbody>
</table>

<p><strong>bfloat16 선택 이유:</strong></p>
<ul>
  <li><strong>메모리 절약</strong>: 50% 메모리 감소</li>
  <li><strong>속도 향상</strong>: 1.5-2배 빠른 연산</li>
  <li><strong>안정성</strong>: float16보다 넓은 지수 범위</li>
  <li><strong>하드웨어 지원</strong>: 최신 GPU에서 네이티브 지원</li>
</ul>

<h3 id="3-그래디언트-및-추론-최적화">3. 그래디언트 및 추론 최적화</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 추론 모드 활성화
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">inference_mode</span><span class="p">():</span>
    <span class="c1"># 그래디언트 계산 완전 비활성화
</span>    <span class="n">samples</span> <span class="o">=</span> <span class="n">scheduler</span><span class="p">.</span><span class="n">sample</span><span class="p">(...)</span>

<span class="c1"># 모델 평가 모드
</span><span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">().</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># CUDA 캐시 정리
</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">empty_cache</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>torch.inference_mode() vs torch.no_grad()</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.no_grad(): 그래디언트 계산만 비활성화
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="c1"># torch.inference_mode(): 완전한 추론 최적화
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">inference_mode</span><span class="p">():</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>  <span class="c1"># 더 빠름, 메모리 절약
</span></code></pre></div></div>

<h2 id="flash-attention-최적화">Flash Attention 최적화</h2>

<h3 id="1-flash-attention-2-설치-및-활용">1. Flash Attention 2 설치 및 활용</h3>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Dockerfile에서 Flash Attention 설치</span>
<span class="k">RUN </span>pip <span class="nb">install</span> <span class="nt">--no-cache-dir</span> <span class="se">\
</span>    flash-attn<span class="o">==</span>2.2.3.post2 <span class="se">\
</span>    <span class="nt">--no-build-isolation</span> <span class="se">\
</span>    <span class="nt">--config-settings</span><span class="o">=</span><span class="s2">"--global-option=build_ext"</span> <span class="se">\
</span>    <span class="nt">--config-settings</span><span class="o">=</span><span class="s2">"--global-option=-j1"</span> <span class="se">\
</span>    <span class="nt">--no-deps</span>
</code></pre></div></div>

<h3 id="2-flash-attention의-성능-향상-원리">2. Flash Attention의 성능 향상 원리</h3>

<h4 id="전통적인-attention의-메모리-복잡도">전통적인 Attention의 메모리 복잡도</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 표준 어텐션: O(n²) 메모리 복잡도
</span><span class="k">def</span> <span class="nf">standard_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="c1"># Q @ K^T: [seq_len, seq_len] 매트릭스 생성
</span>    <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># O(n²) 메모리
</span>    <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>

<h4 id="flash-attention의-최적화">Flash Attention의 최적화</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Flash Attention: O(n) 메모리 복잡도
</span><span class="k">def</span> <span class="nf">flash_attention_concept</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
    <span class="s">"""Flash Attention 핵심 아이디어 (의사코드)"""</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
    
    <span class="c1"># 블록 단위로 처리 (메모리 효율성)
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">block_size</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">block_size</span><span class="p">):</span>
            <span class="c1"># 작은 블록 단위로 어텐션 계산
</span>            <span class="n">q_block</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">block_size</span><span class="p">]</span>
            <span class="n">k_block</span> <span class="o">=</span> <span class="n">K</span><span class="p">[:,</span> <span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">block_size</span><span class="p">]</span>
            <span class="n">v_block</span> <span class="o">=</span> <span class="n">V</span><span class="p">[:,</span> <span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">block_size</span><span class="p">]</span>
            
            <span class="c1"># 블록 내에서만 어텐션 계산
</span>            <span class="n">attention_block</span> <span class="o">=</span> <span class="n">compute_attention_block</span><span class="p">(</span><span class="n">q_block</span><span class="p">,</span> <span class="n">k_block</span><span class="p">,</span> <span class="n">v_block</span><span class="p">)</span>
            <span class="n">output</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">block_size</span><span class="p">]</span> <span class="o">+=</span> <span class="n">attention_block</span>
    
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>

<h4 id="성능-향상-결과">성능 향상 결과</h4>

<table>
  <thead>
    <tr>
      <th>시퀀스 길이</th>
      <th>표준 Attention</th>
      <th>Flash Attention</th>
      <th>메모리 절약</th>
      <th>속도 향상</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>512</td>
      <td>100%</td>
      <td>45%</td>
      <td>55%</td>
      <td>2.2x</td>
    </tr>
    <tr>
      <td>1024</td>
      <td>100%</td>
      <td>25%</td>
      <td>75%</td>
      <td>4.0x</td>
    </tr>
    <tr>
      <td>2048</td>
      <td>100%</td>
      <td>15%</td>
      <td>85%</td>
      <td>6.7x</td>
    </tr>
    <tr>
      <td>4096</td>
      <td>100%</td>
      <td>8%</td>
      <td>92%</td>
      <td>12.5x</td>
    </tr>
  </tbody>
</table>

<h3 id="3-flash-attention-3-고급-최적화">3. Flash Attention 3 고급 최적화</h3>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 선택적으로 Flash Attention 3 설치</span>
<span class="k">RUN </span>git clone https://github.com/Dao-AILab/flash-attention <span class="o">&amp;&amp;</span> <span class="se">\
</span>    <span class="nb">cd </span>flash-attention/hopper <span class="o">&amp;&amp;</span> <span class="se">\
</span>    python setup.py <span class="nb">install</span>
</code></pre></div></div>

<p><strong>Flash Attention 3의 추가 개선사항:</strong></p>
<ul>
  <li><strong>H100 GPU 최적화</strong>: Hopper 아키텍처 전용 최적화</li>
  <li><strong>더 긴 시퀀스 지원</strong>: 32K+ 토큰 처리 가능</li>
  <li><strong>낮은 정밀도 지원</strong>: FP8 연산 활용</li>
</ul>

<h2 id="xformers-메모리-최적화">xformers 메모리 최적화</h2>

<h3 id="1-xformers-설치-및-설정">1. xformers 설치 및 설정</h3>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 메모리 최적화를 위한 xformers 설치</span>
<span class="k">RUN </span>pip <span class="nb">install</span> <span class="nt">--no-cache-dir</span> <span class="nv">xformers</span><span class="o">==</span>0.0.27.post2 <span class="nt">--no-build-isolation</span>
</code></pre></div></div>

<h3 id="2-xformers-최적화-기법">2. xformers 최적화 기법</h3>

<h4 id="memory-efficient-attention">Memory-Efficient Attention</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">xformers.ops</span> <span class="k">as</span> <span class="n">xops</span>

<span class="k">def</span> <span class="nf">memory_efficient_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="s">"""xformers의 메모리 효율적 어텐션"""</span>
    <span class="k">return</span> <span class="n">xops</span><span class="p">.</span><span class="n">memory_efficient_attention</span><span class="p">(</span>
        <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span>
        <span class="n">attn_bias</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="p">)</span>
</code></pre></div></div>

<h4 id="checkpointing-최적화">Checkpointing 최적화</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">xformers.checkpoint</span> <span class="k">as</span> <span class="n">checkpoint</span>

<span class="k">class</span> <span class="nc">MemoryEfficientTransformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># 메모리 절약을 위한 그래디언트 체크포인팅
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">.</span><span class="n">checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">.</span><span class="n">checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer2</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<h3 id="3-성능-비교">3. 성능 비교</h3>

<table>
  <thead>
    <tr>
      <th>최적화 기법</th>
      <th>메모리 사용량</th>
      <th>속도</th>
      <th>구현 복잡도</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>기본 PyTorch</strong></td>
      <td>100%</td>
      <td>1.0x</td>
      <td>⭐</td>
    </tr>
    <tr>
      <td><strong>xformers</strong></td>
      <td>60-70%</td>
      <td>1.3-1.5x</td>
      <td>⭐⭐</td>
    </tr>
    <tr>
      <td><strong>Flash Attention</strong></td>
      <td>40-50%</td>
      <td>2.0-3.0x</td>
      <td>⭐⭐⭐</td>
    </tr>
    <tr>
      <td><strong>Flash + xformers</strong></td>
      <td>30-40%</td>
      <td>2.5-4.0x</td>
      <td>⭐⭐⭐⭐</td>
    </tr>
  </tbody>
</table>

<h2 id="분산-처리-최적화">분산 처리 최적화</h2>

<h3 id="1-nccl-통신-최적화">1. NCCL 통신 최적화</h3>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 분산 훈련을 위한 NCCL 설정</span>
<span class="k">ENV</span><span class="s"> NCCL_DEBUG=INFO</span>
<span class="k">ENV</span><span class="s"> NCCL_IB_DISABLE=1</span>
<span class="k">ENV</span><span class="s"> NCCL_P2P_DISABLE=1</span>
<span class="k">ENV</span><span class="s"> NCCL_SOCKET_IFNAME=eth0</span>
<span class="k">ENV</span><span class="s"> NCCL_BLOCKING_WAIT=1</span>
</code></pre></div></div>

<h4 id="각-설정의-의미">각 설정의 의미</h4>

<p><strong>NCCL_IB_DISABLE=1</strong></p>
<ul>
  <li><strong>목적</strong>: InfiniBand 비활성화</li>
  <li><strong>상황</strong>: 일반 이더넷 환경</li>
  <li><strong>효과</strong>: 통신 안정성 향상</li>
</ul>

<p><strong>NCCL_P2P_DISABLE=1</strong></p>
<ul>
  <li><strong>목적</strong>: P2P 통신 비활성화</li>
  <li><strong>상황</strong>: 다중 노드 환경</li>
  <li><strong>효과</strong>: 네트워크 병목 방지</li>
</ul>

<h3 id="2-멀티-gpu-추론-전략">2. 멀티 GPU 추론 전략</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 단일 GPU (256px)</span>
torchrun <span class="nt">--nproc_per_node</span> 1 <span class="nt">--standalone</span> scripts/diffusion/inference.py <span class="se">\</span>
    configs/diffusion/inference/t2i2v_256px.py <span class="se">\</span>
    <span class="nt">--save-dir</span> samples <span class="nt">--prompt</span> <span class="s2">"raining, sea"</span>

<span class="c"># 8 GPU 분산 처리 (768px)</span>
torchrun <span class="nt">--nproc_per_node</span> 8 <span class="nt">--standalone</span> scripts/diffusion/inference.py <span class="se">\</span>
    configs/diffusion/inference/t2i2v_768px.py <span class="se">\</span>
    <span class="nt">--save-dir</span> samples <span class="nt">--prompt</span> <span class="s2">"raining, sea"</span>
</code></pre></div></div>

<h4 id="gpu별-성능-확장성">GPU별 성능 확장성</h4>

<table>
  <thead>
    <tr>
      <th>GPU 수</th>
      <th>256px (시간/메모리)</th>
      <th>768px (시간/메모리)</th>
      <th>확장 효율성</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1 GPU</td>
      <td>60s / 52.5GB</td>
      <td>1656s / 60.3GB</td>
      <td>-</td>
    </tr>
    <tr>
      <td>2 GPU</td>
      <td>40s / 44.3GB</td>
      <td>863s / 48.3GB</td>
      <td>75%</td>
    </tr>
    <tr>
      <td>4 GPU</td>
      <td>34s / 44.3GB</td>
      <td>466s / 44.3GB</td>
      <td>85%</td>
    </tr>
    <tr>
      <td>8 GPU</td>
      <td>-</td>
      <td>276s / 44.3GB</td>
      <td>90%</td>
    </tr>
  </tbody>
</table>

<h2 id="컴파일-최적화">컴파일 최적화</h2>

<h3 id="1-tensorfloat-32-tf32-활성화">1. TensorFloat-32 (TF32) 활성화</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># TF32 최적화 활성화
</span><span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">matmul</span><span class="p">.</span><span class="n">allow_tf32</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">cudnn</span><span class="p">.</span><span class="n">allow_tf32</span> <span class="o">=</span> <span class="bp">True</span>
</code></pre></div></div>

<p><strong>TF32의 장점:</strong></p>
<ul>
  <li><strong>자동 최적화</strong>: 코드 변경 없이 성능 향상</li>
  <li><strong>A100/H100 최적화</strong>: 최신 GPU에서 2-3배 속도 향상</li>
  <li><strong>정확도 유지</strong>: float32와 거의 동일한 정확도</li>
</ul>

<h3 id="2-jit-컴파일-최적화">2. JIT 컴파일 최적화</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># JIT 컴파일 비활성화 (Gradio 호환성)
</span><span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">_state</span><span class="p">.</span><span class="n">disable</span><span class="p">()</span>

<span class="c1"># 또는 프로덕션에서 JIT 활용
</span><span class="o">@</span><span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">script</span>
<span class="k">def</span> <span class="nf">optimized_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span>
</code></pre></div></div>

<h2 id="동적-최적화-기법">동적 최적화 기법</h2>

<h3 id="1-적응적-배치-크기">1. 적응적 배치 크기</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">adaptive_batch_size</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">max_memory_gb</span><span class="o">=</span><span class="mi">40</span><span class="p">):</span>
    <span class="s">"""GPU 메모리에 따른 적응적 배치 크기 설정"""</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># 테스트 배치로 메모리 사용량 확인
</span>            <span class="n">test_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">*</span><span class="n">input_shape</span><span class="p">).</span><span class="n">cuda</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">inference_mode</span><span class="p">():</span>
                <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>
            
            <span class="c1"># 메모리 사용량 체크
</span>            <span class="n">memory_used</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
            <span class="k">if</span> <span class="n">memory_used</span> <span class="o">&gt;</span> <span class="n">max_memory_gb</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">:</span>  <span class="c1"># 80% 임계값
</span>                <span class="k">break</span>
            
            <span class="n">batch_size</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">except</span> <span class="nb">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">if</span> <span class="s">"out of memory"</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">):</span>
                <span class="n">batch_size</span> <span class="o">-=</span> <span class="mi">1</span>
                <span class="k">break</span>
            <span class="k">raise</span> <span class="n">e</span>
    
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="2-메모리-오프로딩">2. 메모리 오프로딩</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">memory_offloading_inference</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_data</span><span class="p">):</span>
    <span class="s">"""CPU-GPU 메모리 오프로딩"""</span>
    <span class="c1"># 모델을 CPU로 이동
</span>    <span class="n">model</span><span class="p">.</span><span class="n">cpu</span><span class="p">()</span>
    
    <span class="c1"># 필요한 레이어만 GPU로 로드
</span>    <span class="k">for</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="s">"attention"</span> <span class="ow">in</span> <span class="n">layer_name</span><span class="p">:</span>  <span class="c1"># 중요한 레이어만 GPU
</span>            <span class="n">layer</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
    
    <span class="c1"># 추론 실행
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">inference_mode</span><span class="p">():</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_data</span><span class="p">.</span><span class="n">cuda</span><span class="p">())</span>
    
    <span class="c1"># 정리
</span>    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">.</span><span class="n">cpu</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="실시간-모니터링-및-프로파일링">실시간 모니터링 및 프로파일링</h2>

<h3 id="1-gpu-메모리-모니터링">1. GPU 메모리 모니터링</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">monitor_gpu_memory</span><span class="p">():</span>
    <span class="s">"""실시간 GPU 메모리 모니터링"""</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">allocated</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
        <span class="n">reserved</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">memory_reserved</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
        <span class="n">max_allocated</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
        
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"GPU Memory Usage:"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Allocated: </span><span class="si">{</span><span class="n">allocated</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> GB"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Reserved:  </span><span class="si">{</span><span class="n">reserved</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> GB"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Max Used:  </span><span class="si">{</span><span class="n">max_allocated</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> GB"</span><span class="p">)</span>
        
        <span class="c1"># 메모리 사용률이 높으면 경고
</span>        <span class="k">if</span> <span class="n">allocated</span> <span class="o">/</span> <span class="n">reserved</span> <span class="o">&gt;</span> <span class="mf">0.9</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"⚠️  GPU memory usage is high!"</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="p">{</span>
            <span class="s">'allocated'</span><span class="p">:</span> <span class="n">allocated</span><span class="p">,</span>
            <span class="s">'reserved'</span><span class="p">:</span> <span class="n">reserved</span><span class="p">,</span>
            <span class="s">'max_allocated'</span><span class="p">:</span> <span class="n">max_allocated</span>
        <span class="p">}</span>
</code></pre></div></div>

<h3 id="2-성능-프로파일링">2. 성능 프로파일링</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">profile_model_performance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">num_runs</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="s">"""모델 성능 프로파일링"""</span>
    <span class="kn">import</span> <span class="nn">time</span>
    
    <span class="c1"># 워밍업
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">inference_mode</span><span class="p">():</span>
            <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
    
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">synchronize</span><span class="p">()</span>
    
    <span class="c1"># 실제 측정
</span>    <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_runs</span><span class="p">):</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
        
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">inference_mode</span><span class="p">():</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
        
        <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
        
        <span class="n">times</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Run </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">times</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">s"</span><span class="p">)</span>
    
    <span class="n">avg_time</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">times</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>
    <span class="n">std_time</span> <span class="o">=</span> <span class="p">(</span><span class="nb">sum</span><span class="p">((</span><span class="n">t</span> <span class="o">-</span> <span class="n">avg_time</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">times</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">times</span><span class="p">))</span><span class="o">**</span><span class="mf">0.5</span>
    
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Average: </span><span class="si">{</span><span class="n">avg_time</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">s ± </span><span class="si">{</span><span class="n">std_time</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">s"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">avg_time</span><span class="p">,</span> <span class="n">std_time</span>
</code></pre></div></div>

<h2 id="환경별-최적화-가이드">환경별 최적화 가이드</h2>

<h3 id="1-로컬-개발-환경-rtx-30904090">1. 로컬 개발 환경 (RTX 3090/4090)</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 메모리 제한 설정</span>
<span class="nb">export </span><span class="nv">PYTORCH_CUDA_ALLOC_CONF</span><span class="o">=</span><span class="s2">"max_split_size_mb:512,garbage_collection_threshold:0.8"</span>

<span class="c"># 256px 해상도로 제한</span>
python inference.py <span class="nt">--resolution</span> 256px <span class="nt">--length</span> 65 <span class="nt">--offload</span> True
</code></pre></div></div>

<h3 id="2-클라우드-환경-a100h100">2. 클라우드 환경 (A100/H100)</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 최대 성능 설정</span>
<span class="nb">export </span><span class="nv">PYTORCH_CUDA_ALLOC_CONF</span><span class="o">=</span><span class="s2">"max_split_size_mb:8,expandable_segments:True"</span>

<span class="c"># 768px 고해상도 지원</span>
python inference.py <span class="nt">--resolution</span> 768px <span class="nt">--length</span> 97 <span class="nt">--batch_size</span> 2
</code></pre></div></div>

<h3 id="3-멀티-노드-클러스터">3. 멀티 노드 클러스터</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 분산 설정</span>
<span class="nb">export </span><span class="nv">NCCL_DEBUG</span><span class="o">=</span>INFO
<span class="nb">export </span><span class="nv">NCCL_TREE_THRESHOLD</span><span class="o">=</span>0

<span class="c"># 노드 간 통신 최적화</span>
torchrun <span class="nt">--nnodes</span><span class="o">=</span>2 <span class="nt">--nproc_per_node</span><span class="o">=</span>8 <span class="nt">--master_addr</span><span class="o">=</span>192.168.1.100 <span class="se">\</span>
    inference.py <span class="nt">--distributed</span>
</code></pre></div></div>

<h2 id="성능-향상-결과-요약">성능 향상 결과 요약</h2>

<h3 id="최적화-전후-비교">최적화 전후 비교</h3>

<table>
  <thead>
    <tr>
      <th>최적화 기법</th>
      <th>메모리 절약</th>
      <th>속도 향상</th>
      <th>구현 난이도</th>
      <th>안정성</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>bfloat16 변환</strong></td>
      <td>50%</td>
      <td>30-50%</td>
      <td>⭐</td>
      <td>⭐⭐⭐⭐</td>
    </tr>
    <tr>
      <td><strong>Flash Attention</strong></td>
      <td>60-80%</td>
      <td>100-300%</td>
      <td>⭐⭐⭐</td>
      <td>⭐⭐⭐⭐</td>
    </tr>
    <tr>
      <td><strong>xformers</strong></td>
      <td>30-40%</td>
      <td>20-50%</td>
      <td>⭐⭐</td>
      <td>⭐⭐⭐⭐</td>
    </tr>
    <tr>
      <td><strong>CUDA 메모리 최적화</strong></td>
      <td>20-30%</td>
      <td>10-20%</td>
      <td>⭐⭐</td>
      <td>⭐⭐⭐⭐⭐</td>
    </tr>
    <tr>
      <td><strong>분산 처리</strong></td>
      <td>-</td>
      <td>200-600%</td>
      <td>⭐⭐⭐⭐</td>
      <td>⭐⭐⭐</td>
    </tr>
  </tbody>
</table>

<h3 id="실제-벤치마크-결과">실제 벤치마크 결과</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>768px 비디오 생성 (97 프레임, 5초)
├── 최적화 전: 3000s (50분), 80GB 메모리, 1 GPU
├── 부분 최적화: 1656s (28분), 60GB 메모리, 1 GPU
└── 완전 최적화: 276s (5분), 44GB 메모리, 8 GPU

256px 비디오 생성 (97 프레임, 5초)
├── 최적화 전: 120s (2분), 60GB 메모리, 1 GPU
├── 부분 최적화: 60s (1분), 52GB 메모리, 1 GPU
└── 완전 최적화: 34s (34초), 44GB 메모리, 4 GPU
</code></pre></div></div>

<h2 id="결론">결론</h2>

<p>Open-Sora의 성능 최적화는 <strong>다층적 접근법</strong>을 통해 달성됩니다.</p>

<p><strong>핵심 최적화 전략:</strong></p>
<ul>
  <li><strong>메모리 계층 최적화</strong>: CUDA 설정 → 데이터 타입 → 알고리즘</li>
  <li><strong>계산 최적화</strong>: Flash Attention → xformers → 분산 처리</li>
  <li><strong>시스템 최적화</strong>: TF32 → JIT → 하드웨어 특화</li>
</ul>

<p>이러한 최적화 기법들은 다른 대규모 AI 모델에도 적용할 수 있는 범용적 방법론입니다. 특히 메모리 제약이 있는 환경에서 AI 모델을 운영할 때 필수적인 기술들입니다.</p>

<p>다음 포스트에서는 Open-Sora의 실제 사용법과 다양한 활용 예제를 살펴보겠습니다.</p>

<hr />

<p><em>이 글이 도움이 되셨다면 공유해주세요! 궁금한 점이 있으시면 댓글로 남겨주시기 바랍니다.</em></p>

  </div>

  <footer class="post-footer">
    <div class="post-navigation"><div class="nav-previous">
        <a href="/ai-blog/2024/03/29/stable-diffusion-implementation-analysis/" rel="prev">
          <i class="icon-left-arrow"></i>
          <span class="nav-title">Stable Diffusion 구현체 상세 분석 | Detailed Analysis ...</span>
        </a>
      </div><div class="nav-next">
        <a href="/ai-blog/2024/04/01/taming-transformers-implementation-analysis/" rel="next">
          <span class="nav-title">Taming Transformers 구현체 상세 분석 | Detailed Analys...</span>
          <i class="icon-right-arrow"></i>
        </a>
      </div></div>
  </footer>
  </article></div>

    </section>
    <footer class="condensed">
      <ul class="social about-footer condensed"><a href="https://github.com/leeyonghe" target="_blank">
          <li>
            <i class="icon-github-circled"></i>
          </li>
        </a><a href="https://www.linkedin.com/in/lee-yong-hee-18912454/" target="_blank">
          <li>
            <i class="icon-linkedin-squared"></i>
          </li>
        </a><!----></ul><p class="about-footer condensed">&copy;
        2025</p><div class="about-footer condensed">
        <p>Dark Mode
          <i class="icon-moon"></i>
          <label class="switch">
            <input type="checkbox" class="dark-mode-toggle">
            <span class="slider round" onclick="toggleDarkMode()"></span>
          </label>
        </p>
      </div>
    </footer>
  </main>
  
  <script type="text/javascript" src="/ai-blog/assets/js/darkmode.js"></script>
  
  <script src="/ai-blog/assets/js/simple-jekyll-search.min.js"></script>
  <script src="/ai-blog/assets/js/search.js"></script>
  
</body>

</html>
